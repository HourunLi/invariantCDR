nohup: ignoring input
using gpu:0 to train the model
using gpu:0 to train the model
cuda:0
Namespace(A_split=False, JK='sum', a_fold=100, aggregate=False, batch_size=1024, beta=1.5, conv_layers=4, cuda=True, decay_epoch=5, device=device(type='cuda', index=0), device_id='0', domains='cloth_sport', dropout=0.2, epoch=200, feature_dim=126, hidden_dim=126, keep_prob=0.6, lambda_constra=0.15, lambda_critic=0.35, lambda_ease=50, leakey=0.01, load=False, log='logs.txt', log_epoch=5, lr=0.0005, lr_decay=0.95, lr_transfer=0.0005, mask_rate=0.1, min_epoch=50, mode='train', model_file=None, model_name='cs', num_latent_factors=3, num_negative=10, optim='adam', patience=50, proj_layers=1, projection=1, residual=1, save=True, save_dir='./saved_models', seed=32, sim_threshold=0.9, tau=0.4, test_sample_number=999, transfer_epoch=60, user_batch_size=256, weight_decay=0.0002)
number_user 41829
number_item 17943
732772292
49873
/home/hourun/invariantCDR_v2/invariantCDR/utils/GraphMaker.py:46: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
real graph loaded!
Original edge number: 187880; Augmentation edge number: 192213; adding 4333 edges
augmentation graph loaded!
number_user 27328
number_item 12655
333408046
80379
real graph loaded!
Original edge number: 163291; Augmentation edge number: 175271; adding 11980 edges
augmentation graph loaded!
graph loaded!
Loading data from cloth_sport with batch size 1024...
unseen test: 0
test length: 3156
unseen test: 0
test length: 3085
unseen test: 0
test length: 3589
unseen test: 0
test length: 3546
source_user_num 41829
target_user_num 27328
source_item_num 17943
target_item_num 12655
shared users id: 7857
test users 990, 982
unseen test: 0
dev length: 1000
unseen test: 0
dev length: 1000
source train data : 187880, target train data: 163291, source test data : 3085, target test data : 3546, source dev data : 1000, target dev data : 1000
2024-07-04 14:17:27.450095: step 343/68600 (epoch 1/200), loss = 3.072469 (47.636 sec/epoch), lr: 0.000500
2024-07-04 14:18:14.516888: step 686/68600 (epoch 2/200), loss = 2.384031 (47.067 sec/epoch), lr: 0.000500
2024-07-04 14:19:01.780119: step 1029/68600 (epoch 3/200), loss = 2.140976 (47.263 sec/epoch), lr: 0.000500
2024-07-04 14:19:48.935712: step 1372/68600 (epoch 4/200), loss = 1.979935 (47.156 sec/epoch), lr: 0.000500
2024-07-04 14:20:36.036254: step 1715/68600 (epoch 5/200), loss = 1.817225 (47.101 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.036110	 ndcg_5: 0.0262	 ndcg_10: 0.0380	 hit@1:0.008000	 hit@5:0.0450	 hit@10: 0.0820
target: 	 mrr: 0.089804	 ndcg_5: 0.0806	 ndcg_10: 0.1014	 hit@1:0.033000	 hit@5:0.1290	 hit@10: 0.1940
new best model saved.

2024-07-04 14:21:23.424266: step 2058/68600 (epoch 6/200), loss = 1.688599 (47.086 sec/epoch), lr: 0.000500
2024-07-04 14:22:10.633338: step 2401/68600 (epoch 7/200), loss = 1.612459 (47.209 sec/epoch), lr: 0.000500
2024-07-04 14:22:57.843729: step 2744/68600 (epoch 8/200), loss = 1.560121 (47.210 sec/epoch), lr: 0.000500
2024-07-04 14:23:45.173155: step 3087/68600 (epoch 9/200), loss = 1.512098 (47.329 sec/epoch), lr: 0.000500
2024-07-04 14:24:32.463643: step 3430/68600 (epoch 10/200), loss = 1.473516 (47.290 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.068105	 ndcg_5: 0.0562	 ndcg_10: 0.0736	 hit@1:0.023000	 hit@5:0.0890	 hit@10: 0.1440
target: 	 mrr: 0.115113	 ndcg_5: 0.1056	 ndcg_10: 0.1332	 hit@1:0.047000	 hit@5:0.1640	 hit@10: 0.2520
new best model saved.

2024-07-04 14:25:19.961621: step 3773/68600 (epoch 11/200), loss = 1.439651 (47.200 sec/epoch), lr: 0.000500
2024-07-04 14:26:07.066311: step 4116/68600 (epoch 12/200), loss = 1.412801 (47.105 sec/epoch), lr: 0.000500
2024-07-04 14:26:54.240681: step 4459/68600 (epoch 13/200), loss = 1.383683 (47.174 sec/epoch), lr: 0.000500
2024-07-04 14:27:41.516735: step 4802/68600 (epoch 14/200), loss = 1.363349 (47.276 sec/epoch), lr: 0.000500
2024-07-04 14:28:28.983850: step 5145/68600 (epoch 15/200), loss = 1.345302 (47.467 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.112299	 ndcg_5: 0.0989	 ndcg_10: 0.1238	 hit@1:0.052000	 hit@5:0.1480	 hit@10: 0.2250
target: 	 mrr: 0.059998	 ndcg_5: 0.0477	 ndcg_10: 0.0602	 hit@1:0.023000	 hit@5:0.0720	 hit@10: 0.1110

2024-07-04 14:29:16.414811: step 5488/68600 (epoch 16/200), loss = 1.325506 (47.239 sec/epoch), lr: 0.000475
2024-07-04 14:30:03.480701: step 5831/68600 (epoch 17/200), loss = 1.312464 (47.066 sec/epoch), lr: 0.000475
2024-07-04 14:30:50.536406: step 6174/68600 (epoch 18/200), loss = 1.298398 (47.056 sec/epoch), lr: 0.000475
2024-07-04 14:31:37.871810: step 6517/68600 (epoch 19/200), loss = 1.289973 (47.335 sec/epoch), lr: 0.000475
2024-07-04 14:32:24.996984: step 6860/68600 (epoch 20/200), loss = 1.276992 (47.125 sec/epoch), lr: 0.000475
Evaluating on dev set...
....................
source: 	 mrr: 0.102906	 ndcg_5: 0.0925	 ndcg_10: 0.1144	 hit@1:0.039000	 hit@5:0.1440	 hit@10: 0.2120
target: 	 mrr: 0.118456	 ndcg_5: 0.1048	 ndcg_10: 0.1383	 hit@1:0.044000	 hit@5:0.1640	 hit@10: 0.2680

2024-07-04 14:33:12.528261: step 7203/68600 (epoch 21/200), loss = 1.269054 (47.335 sec/epoch), lr: 0.000475
2024-07-04 14:33:59.563411: step 7546/68600 (epoch 22/200), loss = 1.259565 (47.035 sec/epoch), lr: 0.000475
2024-07-04 14:34:46.631081: step 7889/68600 (epoch 23/200), loss = 1.252801 (47.068 sec/epoch), lr: 0.000475
2024-07-04 14:35:33.695373: step 8232/68600 (epoch 24/200), loss = 1.243089 (47.064 sec/epoch), lr: 0.000475
2024-07-04 14:36:20.728129: step 8575/68600 (epoch 25/200), loss = 1.238188 (47.033 sec/epoch), lr: 0.000475
Evaluating on dev set...
....................
source: 	 mrr: 0.098737	 ndcg_5: 0.0873	 ndcg_10: 0.1100	 hit@1:0.034000	 hit@5:0.1370	 hit@10: 0.2070
target: 	 mrr: 0.057931	 ndcg_5: 0.0480	 ndcg_10: 0.0605	 hit@1:0.019000	 hit@5:0.0790	 hit@10: 0.1180

2024-07-04 14:37:07.971102: step 8918/68600 (epoch 26/200), loss = 1.228175 (47.051 sec/epoch), lr: 0.000451
2024-07-04 14:37:54.997591: step 9261/68600 (epoch 27/200), loss = 1.220889 (47.026 sec/epoch), lr: 0.000451
2024-07-04 14:38:42.164757: step 9604/68600 (epoch 28/200), loss = 1.218001 (47.167 sec/epoch), lr: 0.000451
2024-07-04 14:39:29.286016: step 9947/68600 (epoch 29/200), loss = 1.213150 (47.121 sec/epoch), lr: 0.000451
2024-07-04 14:40:16.443431: step 10290/68600 (epoch 30/200), loss = 1.206076 (47.157 sec/epoch), lr: 0.000451
Evaluating on dev set...
....................
source: 	 mrr: 0.105228	 ndcg_5: 0.0950	 ndcg_10: 0.1178	 hit@1:0.042000	 hit@5:0.1490	 hit@10: 0.2200
target: 	 mrr: 0.136498	 ndcg_5: 0.1217	 ndcg_10: 0.1573	 hit@1:0.056000	 hit@5:0.1850	 hit@10: 0.2950
new best model saved.

2024-07-04 14:41:04.271198: step 10633/68600 (epoch 31/200), loss = 1.203943 (47.542 sec/epoch), lr: 0.000451
2024-07-04 14:41:51.537768: step 10976/68600 (epoch 32/200), loss = 1.200580 (47.267 sec/epoch), lr: 0.000451
2024-07-04 14:42:38.840299: step 11319/68600 (epoch 33/200), loss = 1.194941 (47.302 sec/epoch), lr: 0.000451
2024-07-04 14:43:25.924651: step 11662/68600 (epoch 34/200), loss = 1.189399 (47.084 sec/epoch), lr: 0.000451
2024-07-04 14:44:13.679357: step 12005/68600 (epoch 35/200), loss = 1.185714 (47.755 sec/epoch), lr: 0.000451
Evaluating on dev set...
....................
source: 	 mrr: 0.106619	 ndcg_5: 0.0957	 ndcg_10: 0.1197	 hit@1:0.046000	 hit@5:0.1430	 hit@10: 0.2170
target: 	 mrr: 0.109640	 ndcg_5: 0.0948	 ndcg_10: 0.1248	 hit@1:0.042000	 hit@5:0.1480	 hit@10: 0.2410

2024-07-04 14:45:00.935324: step 12348/68600 (epoch 36/200), loss = 1.177605 (47.064 sec/epoch), lr: 0.000429
2024-07-04 14:45:48.041248: step 12691/68600 (epoch 37/200), loss = 1.172978 (47.106 sec/epoch), lr: 0.000429
2024-07-04 14:46:35.105649: step 13034/68600 (epoch 38/200), loss = 1.170255 (47.064 sec/epoch), lr: 0.000429
2024-07-04 14:47:22.184832: step 13377/68600 (epoch 39/200), loss = 1.169938 (47.079 sec/epoch), lr: 0.000429
2024-07-04 14:48:09.243468: step 13720/68600 (epoch 40/200), loss = 1.164215 (47.059 sec/epoch), lr: 0.000429
Evaluating on dev set...
....................
source: 	 mrr: 0.199679	 ndcg_5: 0.1893	 ndcg_10: 0.2317	 hit@1:0.101000	 hit@5:0.2740	 hit@10: 0.4050
target: 	 mrr: 0.150954	 ndcg_5: 0.1420	 ndcg_10: 0.1784	 hit@1:0.059000	 hit@5:0.2220	 hit@10: 0.3350
new best model saved.

2024-07-04 14:48:56.664523: step 14063/68600 (epoch 41/200), loss = 1.163459 (47.117 sec/epoch), lr: 0.000429
2024-07-04 14:49:43.804728: step 14406/68600 (epoch 42/200), loss = 1.158207 (47.140 sec/epoch), lr: 0.000429
2024-07-04 14:50:30.958866: step 14749/68600 (epoch 43/200), loss = 1.154307 (47.154 sec/epoch), lr: 0.000429
2024-07-04 14:51:18.106662: step 15092/68600 (epoch 44/200), loss = 1.152374 (47.148 sec/epoch), lr: 0.000429
2024-07-04 14:52:05.289563: step 15435/68600 (epoch 45/200), loss = 1.148907 (47.183 sec/epoch), lr: 0.000429
Evaluating on dev set...
....................
source: 	 mrr: 0.159070	 ndcg_5: 0.1491	 ndcg_10: 0.1805	 hit@1:0.079000	 hit@5:0.2180	 hit@10: 0.3160
target: 	 mrr: 0.095969	 ndcg_5: 0.0804	 ndcg_10: 0.1117	 hit@1:0.030000	 hit@5:0.1310	 hit@10: 0.2280

2024-07-04 14:52:52.757922: step 15778/68600 (epoch 46/200), loss = 1.144136 (47.275 sec/epoch), lr: 0.000407
2024-07-04 14:53:40.137028: step 16121/68600 (epoch 47/200), loss = 1.140656 (47.379 sec/epoch), lr: 0.000407
2024-07-04 14:54:27.382782: step 16464/68600 (epoch 48/200), loss = 1.138824 (47.246 sec/epoch), lr: 0.000407
2024-07-04 14:55:14.743197: step 16807/68600 (epoch 49/200), loss = 1.137174 (47.360 sec/epoch), lr: 0.000407
2024-07-04 14:56:01.887963: step 17150/68600 (epoch 50/200), loss = 1.136463 (47.145 sec/epoch), lr: 0.000407
Evaluating on dev set...
....................
source: 	 mrr: 0.166797	 ndcg_5: 0.1583	 ndcg_10: 0.1927	 hit@1:0.082000	 hit@5:0.2330	 hit@10: 0.3390
target: 	 mrr: 0.168114	 ndcg_5: 0.1627	 ndcg_10: 0.2050	 hit@1:0.066000	 hit@5:0.2610	 hit@10: 0.3930

2024-07-04 14:56:49.156266: step 17493/68600 (epoch 51/200), loss = 1.134502 (47.075 sec/epoch), lr: 0.000407
2024-07-04 14:57:36.248569: step 17836/68600 (epoch 52/200), loss = 1.131175 (47.092 sec/epoch), lr: 0.000407
2024-07-04 14:58:23.286638: step 18179/68600 (epoch 53/200), loss = 1.128981 (47.038 sec/epoch), lr: 0.000407
2024-07-04 14:59:10.384411: step 18522/68600 (epoch 54/200), loss = 1.126523 (47.098 sec/epoch), lr: 0.000407
2024-07-04 14:59:57.500062: step 18865/68600 (epoch 55/200), loss = 1.126246 (47.116 sec/epoch), lr: 0.000407
Evaluating on dev set...
....................
source: 	 mrr: 0.182773	 ndcg_5: 0.1799	 ndcg_10: 0.2204	 hit@1:0.079000	 hit@5:0.2760	 hit@10: 0.4030
target: 	 mrr: 0.101308	 ndcg_5: 0.0863	 ndcg_10: 0.1127	 hit@1:0.045000	 hit@5:0.1280	 hit@10: 0.2100

2024-07-04 15:00:45.160234: step 19208/68600 (epoch 56/200), loss = 1.121696 (47.468 sec/epoch), lr: 0.000387
2024-07-04 15:01:32.206109: step 19551/68600 (epoch 57/200), loss = 1.119633 (47.046 sec/epoch), lr: 0.000387
2024-07-04 15:02:19.298919: step 19894/68600 (epoch 58/200), loss = 1.114751 (47.093 sec/epoch), lr: 0.000387
2024-07-04 15:03:06.619578: step 20237/68600 (epoch 59/200), loss = 1.113229 (47.321 sec/epoch), lr: 0.000387
2024-07-04 15:03:53.972773: step 20580/68600 (epoch 60/200), loss = 1.112559 (47.353 sec/epoch), lr: 0.000387
Evaluating on dev set...
....................
source: 	 mrr: 0.160856	 ndcg_5: 0.1507	 ndcg_10: 0.1840	 hit@1:0.074000	 hit@5:0.2250	 hit@10: 0.3280
target: 	 mrr: 0.251243	 ndcg_5: 0.2480	 ndcg_10: 0.2991	 hit@1:0.127000	 hit@5:0.3610	 hit@10: 0.5190
new best model saved.
shift to transfer learning mode

2024-07-04 15:07:16.866053: step 20923/68600 (epoch 61/200), loss = 1.375355 (202.557 sec/epoch), lr: 0.000500
2024-07-04 15:10:39.133086: step 21266/68600 (epoch 62/200), loss = 1.317851 (202.267 sec/epoch), lr: 0.000500
2024-07-04 15:14:01.408851: step 21609/68600 (epoch 63/200), loss = 1.287923 (202.276 sec/epoch), lr: 0.000500
2024-07-04 15:17:23.597356: step 21952/68600 (epoch 64/200), loss = 1.269025 (202.188 sec/epoch), lr: 0.000500
2024-07-04 15:20:45.820201: step 22295/68600 (epoch 65/200), loss = 1.252147 (202.223 sec/epoch), lr: 0.000500
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.051503	 ndcg_5: 0.0426	 ndcg_10: 0.0541	 hit@1:0.020596	 hit@5:0.0640	 hit@10: 0.1001
target: 	 mrr: 0.040781	 ndcg_5: 0.0339	 ndcg_10: 0.0417	 hit@1:0.015046	 hit@5:0.0529	 hit@10: 0.0775
epoch 65: train_loss = 1.252147, source_hit@10 = 0.1001, source_ndcg@10 = 0.0541, target_hit@10 = 0.0775, target_ndcg@10 = 0.0417

2024-07-04 15:24:08.968065: step 22638/68600 (epoch 66/200), loss = 1.253188 (202.326 sec/epoch), lr: 0.000475
2024-07-04 15:27:31.156599: step 22981/68600 (epoch 67/200), loss = 1.246919 (202.188 sec/epoch), lr: 0.000475
2024-07-04 15:30:53.359264: step 23324/68600 (epoch 68/200), loss = 1.236604 (202.203 sec/epoch), lr: 0.000475
2024-07-04 15:34:15.570078: step 23667/68600 (epoch 69/200), loss = 1.229276 (202.211 sec/epoch), lr: 0.000475
2024-07-04 15:37:37.736021: step 24010/68600 (epoch 70/200), loss = 1.225365 (202.166 sec/epoch), lr: 0.000475
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.048202	 ndcg_5: 0.0397	 ndcg_10: 0.0512	 hit@1:0.017110	 hit@5:0.0624	 hit@10: 0.0982
target: 	 mrr: 0.037629	 ndcg_5: 0.0297	 ndcg_10: 0.0383	 hit@1:0.013931	 hit@5:0.0451	 hit@10: 0.0716
epoch 70: train_loss = 1.225365, source_hit@10 = 0.0982, source_ndcg@10 = 0.0512, target_hit@10 = 0.0716, target_ndcg@10 = 0.0383

2024-07-04 15:41:01.237539: step 24353/68600 (epoch 71/200), loss = 1.215313 (202.675 sec/epoch), lr: 0.000451
2024-07-04 15:44:23.922876: step 24696/68600 (epoch 72/200), loss = 1.211608 (202.685 sec/epoch), lr: 0.000451
2024-07-04 15:47:46.494711: step 25039/68600 (epoch 73/200), loss = 1.208246 (202.572 sec/epoch), lr: 0.000451
2024-07-04 15:51:09.176336: step 25382/68600 (epoch 74/200), loss = 1.204811 (202.682 sec/epoch), lr: 0.000451
2024-07-04 15:54:31.986892: step 25725/68600 (epoch 75/200), loss = 1.202041 (202.811 sec/epoch), lr: 0.000451
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.046726	 ndcg_5: 0.0356	 ndcg_10: 0.0501	 hit@1:0.016160	 hit@5:0.0542	 hit@10: 0.0992
target: 	 mrr: 0.034927	 ndcg_5: 0.0277	 ndcg_10: 0.0347	 hit@1:0.011702	 hit@5:0.0435	 hit@10: 0.0649
epoch 75: train_loss = 1.202041, source_hit@10 = 0.0992, source_ndcg@10 = 0.0501, target_hit@10 = 0.0649, target_ndcg@10 = 0.0347

