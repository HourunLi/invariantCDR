nohup: ignoring input
using gpu:0 to train the model
using gpu:0 to train the model
cuda:0
Namespace(A_split=False, JK='sum', a_fold=100, aggregate=False, batch_size=1024, beta=1.5, conv_layers=3, cuda=True, decay_epoch=5, device=device(type='cuda', index=0), device_id='0', domains='cloth_sport', dropout=0.2, epoch=200, feature_dim=126, hidden_dim=126, keep_prob=0.6, lambda_constra=0.15, lambda_critic=0.35, lambda_ease=50, leakey=0.01, load=True, log='logs.txt', log_epoch=5, lr=0.0005, lr_decay=0.95, lr_transfer=0.0005, mask_rate=0.1, min_epoch=50, mode='train', model_file='checkpoint_epoch_40.pt', model_name='cs', num_latent_factors=3, num_negative=10, optim='adam', patience=50, proj_layers=1, projection=1, residual=1, save=True, save_dir='./saved_models', seed=32, sim_threshold=0.9, tau=0.4, test_sample_number=999, transfer_epoch=40, user_batch_size=256, weight_decay=0.0002)
number_user 41829
number_item 17943
732772292
49873
/home/hourun/invariantCDR_v2/invariantCDR/utils/GraphMaker.py:47: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
real graph loaded!
Original edge number: 187880; Augmentation edge number: 192213; adding 4333 edges
augmentation graph loaded!
number_user 27328
number_item 12655
333408046
80379
real graph loaded!
Original edge number: 163291; Augmentation edge number: 175271; adding 11980 edges
augmentation graph loaded!
graph loaded!
Loading data from cloth_sport with batch size 1024...
unseen test: 0
test length: 3156
unseen test: 0
test length: 3085
unseen test: 0
test length: 3589
unseen test: 0
test length: 3546
source_user_num 41829
target_user_num 27328
source_item_num 17943
target_item_num 12655
shared users id: 7857
test users 990, 982
unseen test: 0
dev length: 1000
unseen test: 0
dev length: 1000
source train data : 187880, target train data: 163291, source test data : 3085, target test data : 3546, source dev data : 1000, target dev data : 1000
Loading model from ./saved_models/cs/checkpoint_epoch_40.pt
2024-07-03 11:25:44.270873: step 343/68600 (epoch 40/200), loss = 1.388357 (195.221 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.177409	 ndcg_5: 0.1706	 ndcg_10: 0.2103	 hit@1:0.081000	 hit@5:0.2560	 hit@10: 0.3800
target: 	 mrr: 0.071351	 ndcg_5: 0.0609	 ndcg_10: 0.0768	 hit@1:0.031000	 hit@5:0.0900	 hit@10: 0.1400
new best model saved.

2024-07-03 11:29:00.087809: step 686/68600 (epoch 41/200), loss = 1.355103 (195.283 sec/epoch), lr: 0.000500
2024-07-03 11:32:15.928534: step 1029/68600 (epoch 42/200), loss = 1.331715 (195.841 sec/epoch), lr: 0.000500
2024-07-03 11:35:31.827397: step 1372/68600 (epoch 43/200), loss = 1.310888 (195.899 sec/epoch), lr: 0.000500
2024-07-03 11:38:47.712377: step 1715/68600 (epoch 44/200), loss = 1.295564 (195.885 sec/epoch), lr: 0.000500
2024-07-03 11:42:03.655063: step 2058/68600 (epoch 45/200), loss = 1.282971 (195.943 sec/epoch), lr: 0.000500
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.047968	 ndcg_5: 0.0401	 ndcg_10: 0.0506	 hit@1:0.019962	 hit@5:0.0615	 hit@10: 0.0941
target: 	 mrr: 0.036758	 ndcg_5: 0.0286	 ndcg_10: 0.0380	 hit@1:0.011145	 hit@5:0.0454	 hit@10: 0.0744
epoch 45: train_loss = 1.282971, source_hit@10 = 0.0941, source_ndcg@10 = 0.0506, target_hit@10 = 0.0744, target_ndcg@10 = 0.0380

2024-07-03 11:45:19.976908: step 2401/68600 (epoch 46/200), loss = 1.269035 (195.498 sec/epoch), lr: 0.000475
2024-07-03 11:48:35.427602: step 2744/68600 (epoch 47/200), loss = 1.258374 (195.451 sec/epoch), lr: 0.000475
2024-07-03 11:51:50.899369: step 3087/68600 (epoch 48/200), loss = 1.250249 (195.472 sec/epoch), lr: 0.000475
2024-07-03 11:55:06.254989: step 3430/68600 (epoch 49/200), loss = 1.244464 (195.356 sec/epoch), lr: 0.000475
2024-07-03 11:58:21.753268: step 3773/68600 (epoch 50/200), loss = 1.237930 (195.498 sec/epoch), lr: 0.000475
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.047070	 ndcg_5: 0.0381	 ndcg_10: 0.0497	 hit@1:0.018695	 hit@5:0.0577	 hit@10: 0.0938
target: 	 mrr: 0.037650	 ndcg_5: 0.0297	 ndcg_10: 0.0377	 hit@1:0.015325	 hit@5:0.0446	 hit@10: 0.0691
epoch 50: train_loss = 1.237930, source_hit@10 = 0.0938, source_ndcg@10 = 0.0497, target_hit@10 = 0.0691, target_ndcg@10 = 0.0377

2024-07-03 12:01:38.033863: step 4116/68600 (epoch 51/200), loss = 1.230661 (195.459 sec/epoch), lr: 0.000451
2024-07-03 12:04:53.809560: step 4459/68600 (epoch 52/200), loss = 1.224031 (195.776 sec/epoch), lr: 0.000451
2024-07-03 12:08:09.851330: step 4802/68600 (epoch 53/200), loss = 1.218503 (196.042 sec/epoch), lr: 0.000451
2024-07-03 12:11:25.865468: step 5145/68600 (epoch 54/200), loss = 1.214864 (196.014 sec/epoch), lr: 0.000451
2024-07-03 12:14:41.880595: step 5488/68600 (epoch 55/200), loss = 1.211582 (196.015 sec/epoch), lr: 0.000451
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.045849	 ndcg_5: 0.0380	 ndcg_10: 0.0490	 hit@1:0.016477	 hit@5:0.0593	 hit@10: 0.0938
target: 	 mrr: 0.043691	 ndcg_5: 0.0352	 ndcg_10: 0.0476	 hit@1:0.012260	 hit@5:0.0574	 hit@10: 0.0958
epoch 55: train_loss = 1.211582, source_hit@10 = 0.0938, source_ndcg@10 = 0.0490, target_hit@10 = 0.0958, target_ndcg@10 = 0.0476

2024-07-03 12:17:58.211833: step 5831/68600 (epoch 56/200), loss = 1.211855 (195.510 sec/epoch), lr: 0.000451
2024-07-03 12:21:13.661334: step 6174/68600 (epoch 57/200), loss = 1.206166 (195.449 sec/epoch), lr: 0.000451
2024-07-03 12:24:29.257592: step 6517/68600 (epoch 58/200), loss = 1.206199 (195.596 sec/epoch), lr: 0.000451
2024-07-03 12:27:45.252223: step 6860/68600 (epoch 59/200), loss = 1.199267 (195.995 sec/epoch), lr: 0.000451
2024-07-03 12:31:01.288429: step 7203/68600 (epoch 60/200), loss = 1.195498 (196.036 sec/epoch), lr: 0.000451
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.033544	 ndcg_5: 0.0259	 ndcg_10: 0.0340	 hit@1:0.010456	 hit@5:0.0409	 hit@10: 0.0659
target: 	 mrr: 0.037929	 ndcg_5: 0.0307	 ndcg_10: 0.0398	 hit@1:0.013931	 hit@5:0.0474	 hit@10: 0.0755
epoch 60: train_loss = 1.195498, source_hit@10 = 0.0659, source_ndcg@10 = 0.0340, target_hit@10 = 0.0755, target_ndcg@10 = 0.0398

2024-07-03 12:34:18.120498: step 7546/68600 (epoch 61/200), loss = 1.189496 (196.011 sec/epoch), lr: 0.000429
2024-07-03 12:37:34.025879: step 7889/68600 (epoch 62/200), loss = 1.188937 (195.905 sec/epoch), lr: 0.000429
2024-07-03 12:40:49.913434: step 8232/68600 (epoch 63/200), loss = 1.186782 (195.888 sec/epoch), lr: 0.000429
2024-07-03 12:44:05.701364: step 8575/68600 (epoch 64/200), loss = 1.184041 (195.788 sec/epoch), lr: 0.000429
2024-07-03 12:47:20.879219: step 8918/68600 (epoch 65/200), loss = 1.181598 (195.178 sec/epoch), lr: 0.000429
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.032090	 ndcg_5: 0.0245	 ndcg_10: 0.0325	 hit@1:0.009506	 hit@5:0.0387	 hit@10: 0.0640
target: 	 mrr: 0.042322	 ndcg_5: 0.0332	 ndcg_10: 0.0429	 hit@1:0.016439	 hit@5:0.0504	 hit@10: 0.0805
epoch 65: train_loss = 1.181598, source_hit@10 = 0.0640, source_ndcg@10 = 0.0325, target_hit@10 = 0.0805, target_ndcg@10 = 0.0429

2024-07-03 12:50:36.837940: step 9261/68600 (epoch 66/200), loss = 1.177197 (195.144 sec/epoch), lr: 0.000429
2024-07-03 12:53:52.027861: step 9604/68600 (epoch 67/200), loss = 1.176796 (195.190 sec/epoch), lr: 0.000429
2024-07-03 12:57:07.119871: step 9947/68600 (epoch 68/200), loss = 1.177271 (195.092 sec/epoch), lr: 0.000429
2024-07-03 13:00:22.068736: step 10290/68600 (epoch 69/200), loss = 1.169587 (194.949 sec/epoch), lr: 0.000429
2024-07-03 13:03:37.660007: step 10633/68600 (epoch 70/200), loss = 1.169288 (195.591 sec/epoch), lr: 0.000429
Evaluating on dev set...
..................................................................
source: 	 mrr: 0.039778	 ndcg_5: 0.0320	 ndcg_10: 0.0409	 hit@1:0.013625	 hit@5:0.0516	 hit@10: 0.0798
target: 	 mrr: 0.041601	 ndcg_5: 0.0341	 ndcg_10: 0.0439	 hit@1:0.014767	 hit@5:0.0541	 hit@10: 0.0847
epoch 70: train_loss = 1.169288, source_hit@10 = 0.0798, source_ndcg@10 = 0.0409, target_hit@10 = 0.0847, target_ndcg@10 = 0.0439

2024-07-03 13:06:54.225363: step 10976/68600 (epoch 71/200), loss = 1.172982 (195.744 sec/epoch), lr: 0.000429
2024-07-03 13:10:09.981836: step 11319/68600 (epoch 72/200), loss = 1.167751 (195.756 sec/epoch), lr: 0.000429
2024-07-03 13:13:25.693902: step 11662/68600 (epoch 73/200), loss = 1.167792 (195.712 sec/epoch), lr: 0.000429
