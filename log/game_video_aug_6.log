nohup: ignoring input
using gpu:0 to train the model
using gpu:0 to train the model
cuda:0
Namespace(A_split=False, JK='sum', a_fold=100, aggregate=False, batch_size=1024, beta=1.5, conv_layers=6, cuda=True, decay_epoch=5, device=device(type='cuda', index=0), device_id='0', domains='game_video', dropout=0.2, epoch=70, feature_dim=126, hidden_dim=126, keep_prob=0.6, lambda_constra=0.3, lambda_critic=0.3, lambda_ease=50, leakey=0.01, load=False, log='logs.txt', log_epoch=10, lr=0.0005, lr_decay=0.95, lr_transfer=0.0005, min_epoch=50, mode='train', model_file=None, model_name='gv_6', num_latent_factors=3, num_negative=10, optim='adam', patience=50, proj_layers=1, projection=1, residual=1, save=True, save_dir='./saved_models', seed=32, sim_threshold=0.9, tau=0.4, test_sample_number=999, transfer_epoch=50, user_batch_size=256, weight_decay=0.0002)
number_user 25025
number_item 12319
/home/hourun/invariantCDR_v2/invariantCDR/utils/GraphMaker.py:47: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
real graph loaded!
augmented graph loaded!
number_user 19457
number_item 8751
real graph loaded!
augmented graph loaded!
graph loaded!
Loading data from game_video with batch size 1024...
unseen test: 0
test length: 1381
unseen test: 0
test length: 1304
unseen test: 0
test length: 1435
unseen test: 0
test length: 1458
source_user_num 25025
target_user_num 19457
source_item_num 12319
target_item_num 8751
shared users id: 1737
test users 226, 217
unseen test: 0
dev length: 1000
unseen test: 0
dev length: 1000
source train data : 155036, target train data: 156091, source test data : 1304, target test data : 1458, source dev data : 1000, target dev data : 1000
2024-07-11 11:17:39.264229: step 304/21280 (epoch 1/70), loss = 4.903985 (42.789 sec/epoch), lr: 0.000500
2024-07-11 11:18:21.566383: step 608/21280 (epoch 2/70), loss = 3.786066 (42.302 sec/epoch), lr: 0.000500
2024-07-11 11:19:03.889047: step 912/21280 (epoch 3/70), loss = 3.274628 (42.323 sec/epoch), lr: 0.000500
2024-07-11 11:19:46.204066: step 1216/21280 (epoch 4/70), loss = 2.982844 (42.315 sec/epoch), lr: 0.000500
2024-07-11 11:20:28.515000: step 1520/21280 (epoch 5/70), loss = 2.833374 (42.311 sec/epoch), lr: 0.000500
2024-07-11 11:21:10.817996: step 1824/21280 (epoch 6/70), loss = 2.743285 (42.303 sec/epoch), lr: 0.000500
2024-07-11 11:21:53.116320: step 2128/21280 (epoch 7/70), loss = 2.670662 (42.298 sec/epoch), lr: 0.000500
2024-07-11 11:22:35.421189: step 2432/21280 (epoch 8/70), loss = 2.603733 (42.305 sec/epoch), lr: 0.000500
2024-07-11 11:23:17.743223: step 2736/21280 (epoch 9/70), loss = 2.543338 (42.322 sec/epoch), lr: 0.000500
2024-07-11 11:24:00.065917: step 3040/21280 (epoch 10/70), loss = 2.498742 (42.323 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.070334	 ndcg_5: 0.0589	 ndcg_10: 0.0735	 hit@1:0.028000	 hit@5:0.0910	 hit@10: 0.1360
target: 	 mrr: 0.146830	 ndcg_5: 0.1339	 ndcg_10: 0.1732	 hit@1:0.067000	 hit@5:0.2040	 hit@10: 0.3260
new best model saved.

2024-07-11 11:24:42.650785: step 3344/21280 (epoch 11/70), loss = 2.465987 (42.332 sec/epoch), lr: 0.000500
2024-07-11 11:25:24.973400: step 3648/21280 (epoch 12/70), loss = 2.425958 (42.323 sec/epoch), lr: 0.000500
2024-07-11 11:26:07.307966: step 3952/21280 (epoch 13/70), loss = 2.390635 (42.335 sec/epoch), lr: 0.000500
2024-07-11 11:26:49.648169: step 4256/21280 (epoch 14/70), loss = 2.358722 (42.340 sec/epoch), lr: 0.000500
2024-07-11 11:27:31.975666: step 4560/21280 (epoch 15/70), loss = 2.332548 (42.327 sec/epoch), lr: 0.000500
2024-07-11 11:28:14.698788: step 4864/21280 (epoch 16/70), loss = 2.304976 (42.723 sec/epoch), lr: 0.000500
2024-07-11 11:28:57.004737: step 5168/21280 (epoch 17/70), loss = 2.283507 (42.306 sec/epoch), lr: 0.000500
2024-07-11 11:29:39.304646: step 5472/21280 (epoch 18/70), loss = 2.266235 (42.300 sec/epoch), lr: 0.000500
2024-07-11 11:30:21.612437: step 5776/21280 (epoch 19/70), loss = 2.244023 (42.308 sec/epoch), lr: 0.000500
2024-07-11 11:31:03.919574: step 6080/21280 (epoch 20/70), loss = 2.229154 (42.307 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.125266	 ndcg_5: 0.1096	 ndcg_10: 0.1426	 hit@1:0.051000	 hit@5:0.1650	 hit@10: 0.2670
target: 	 mrr: 0.168247	 ndcg_5: 0.1586	 ndcg_10: 0.1989	 hit@1:0.078000	 hit@5:0.2410	 hit@10: 0.3660
new best model saved.

2024-07-11 11:31:46.504995: step 6384/21280 (epoch 21/70), loss = 2.212636 (42.315 sec/epoch), lr: 0.000500
2024-07-11 11:32:28.807379: step 6688/21280 (epoch 22/70), loss = 2.199446 (42.302 sec/epoch), lr: 0.000500
2024-07-11 11:33:11.115147: step 6992/21280 (epoch 23/70), loss = 2.191058 (42.308 sec/epoch), lr: 0.000500
2024-07-11 11:33:53.425883: step 7296/21280 (epoch 24/70), loss = 2.183460 (42.311 sec/epoch), lr: 0.000500
2024-07-11 11:34:35.724368: step 7600/21280 (epoch 25/70), loss = 2.170946 (42.298 sec/epoch), lr: 0.000500
2024-07-11 11:35:18.018102: step 7904/21280 (epoch 26/70), loss = 2.164489 (42.294 sec/epoch), lr: 0.000500
2024-07-11 11:36:00.316510: step 8208/21280 (epoch 27/70), loss = 2.157726 (42.298 sec/epoch), lr: 0.000500
2024-07-11 11:36:42.616580: step 8512/21280 (epoch 28/70), loss = 2.144813 (42.300 sec/epoch), lr: 0.000500
2024-07-11 11:37:24.916100: step 8816/21280 (epoch 29/70), loss = 2.143292 (42.299 sec/epoch), lr: 0.000500
2024-07-11 11:38:07.210323: step 9120/21280 (epoch 30/70), loss = 2.135242 (42.294 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.156575	 ndcg_5: 0.1465	 ndcg_10: 0.1833	 hit@1:0.067000	 hit@5:0.2240	 hit@10: 0.3380
target: 	 mrr: 0.206350	 ndcg_5: 0.2021	 ndcg_10: 0.2467	 hit@1:0.096000	 hit@5:0.3100	 hit@10: 0.4480
new best model saved.

2024-07-11 11:38:49.793931: step 9424/21280 (epoch 31/70), loss = 2.126781 (42.309 sec/epoch), lr: 0.000500
2024-07-11 11:39:32.098631: step 9728/21280 (epoch 32/70), loss = 2.123198 (42.305 sec/epoch), lr: 0.000500
2024-07-11 11:40:14.405385: step 10032/21280 (epoch 33/70), loss = 2.114410 (42.307 sec/epoch), lr: 0.000500
2024-07-11 11:40:56.710710: step 10336/21280 (epoch 34/70), loss = 2.111554 (42.305 sec/epoch), lr: 0.000500
2024-07-11 11:41:39.416663: step 10640/21280 (epoch 35/70), loss = 2.102963 (42.706 sec/epoch), lr: 0.000500
2024-07-11 11:42:21.708096: step 10944/21280 (epoch 36/70), loss = 2.096809 (42.291 sec/epoch), lr: 0.000500
2024-07-11 11:43:03.987416: step 11248/21280 (epoch 37/70), loss = 2.093063 (42.279 sec/epoch), lr: 0.000500
2024-07-11 11:43:46.265659: step 11552/21280 (epoch 38/70), loss = 2.088003 (42.278 sec/epoch), lr: 0.000500
2024-07-11 11:44:28.538735: step 11856/21280 (epoch 39/70), loss = 2.086362 (42.273 sec/epoch), lr: 0.000500
2024-07-11 11:45:10.814494: step 12160/21280 (epoch 40/70), loss = 2.080476 (42.276 sec/epoch), lr: 0.000500
Evaluating on dev set...
....................
source: 	 mrr: 0.148700	 ndcg_5: 0.1380	 ndcg_10: 0.1783	 hit@1:0.067000	 hit@5:0.2110	 hit@10: 0.3380
target: 	 mrr: 0.201360	 ndcg_5: 0.1915	 ndcg_10: 0.2419	 hit@1:0.093000	 hit@5:0.2910	 hit@10: 0.4480

2024-07-11 11:45:53.288095: step 12464/21280 (epoch 41/70), loss = 2.067777 (42.273 sec/epoch), lr: 0.000475
2024-07-11 11:46:35.616553: step 12768/21280 (epoch 42/70), loss = 2.066788 (42.328 sec/epoch), lr: 0.000475
2024-07-11 11:47:17.886146: step 13072/21280 (epoch 43/70), loss = 2.059365 (42.270 sec/epoch), lr: 0.000475
2024-07-11 11:48:00.181227: step 13376/21280 (epoch 44/70), loss = 2.053861 (42.295 sec/epoch), lr: 0.000475
2024-07-11 11:48:42.477528: step 13680/21280 (epoch 45/70), loss = 2.048705 (42.296 sec/epoch), lr: 0.000475
2024-07-11 11:49:24.814281: step 13984/21280 (epoch 46/70), loss = 2.049528 (42.337 sec/epoch), lr: 0.000475
2024-07-11 11:50:07.119844: step 14288/21280 (epoch 47/70), loss = 2.046697 (42.306 sec/epoch), lr: 0.000475
2024-07-11 11:50:49.421238: step 14592/21280 (epoch 48/70), loss = 2.044905 (42.301 sec/epoch), lr: 0.000475
2024-07-11 11:51:31.723170: step 14896/21280 (epoch 49/70), loss = 2.041618 (42.302 sec/epoch), lr: 0.000475
2024-07-11 11:52:14.006808: step 15200/21280 (epoch 50/70), loss = 2.035989 (42.284 sec/epoch), lr: 0.000475
Evaluating on dev set...
....................
source: 	 mrr: 0.193095	 ndcg_5: 0.1844	 ndcg_10: 0.2297	 hit@1:0.091000	 hit@5:0.2750	 hit@10: 0.4150
target: 	 mrr: 0.224242	 ndcg_5: 0.2162	 ndcg_10: 0.2689	 hit@1:0.108000	 hit@5:0.3220	 hit@10: 0.4860
new best model saved.
shift to transfer learning mode

2024-07-11 11:52:36.801971: step 15504/21280 (epoch 51/70), loss = 0.552531 (22.481 sec/epoch), lr: 0.000500
2024-07-11 11:52:59.318134: step 15808/21280 (epoch 52/70), loss = 0.519245 (22.516 sec/epoch), lr: 0.000500
2024-07-11 11:53:21.795338: step 16112/21280 (epoch 53/70), loss = 0.500174 (22.477 sec/epoch), lr: 0.000500
2024-07-11 11:53:44.674486: step 16416/21280 (epoch 54/70), loss = 0.483120 (22.879 sec/epoch), lr: 0.000500
2024-07-11 11:54:07.146098: step 16720/21280 (epoch 55/70), loss = 0.471294 (22.472 sec/epoch), lr: 0.000500
2024-07-11 11:54:29.622120: step 17024/21280 (epoch 56/70), loss = 0.461335 (22.476 sec/epoch), lr: 0.000500
2024-07-11 11:54:52.101700: step 17328/21280 (epoch 57/70), loss = 0.456609 (22.480 sec/epoch), lr: 0.000500
2024-07-11 11:55:14.587594: step 17632/21280 (epoch 58/70), loss = 0.450910 (22.486 sec/epoch), lr: 0.000500
2024-07-11 11:55:37.068141: step 17936/21280 (epoch 59/70), loss = 0.447124 (22.481 sec/epoch), lr: 0.000500
2024-07-11 11:55:59.546681: step 18240/21280 (epoch 60/70), loss = 0.441409 (22.479 sec/epoch), lr: 0.000500
Evaluating on dev set...
...........................
source: 	 mrr: 0.036694	 ndcg_5: 0.0273	 ndcg_10: 0.0367	 hit@1:0.010138	 hit@5:0.0449	 hit@10: 0.0753
target: 	 mrr: 0.046634	 ndcg_5: 0.0400	 ndcg_10: 0.0492	 hit@1:0.015331	 hit@5:0.0648	 hit@10: 0.0934
epoch 60: train_loss = 0.441409, source_hit@10 = 0.0753, source_ndcg@10 = 0.0367, target_hit@10 = 0.0934, target_ndcg@10 = 0.0492

2024-07-11 11:56:22.299209: step 18544/21280 (epoch 61/70), loss = 0.434735 (22.482 sec/epoch), lr: 0.000475
2024-07-11 11:56:44.778089: step 18848/21280 (epoch 62/70), loss = 0.432738 (22.479 sec/epoch), lr: 0.000475
2024-07-11 11:57:07.267833: step 19152/21280 (epoch 63/70), loss = 0.428562 (22.490 sec/epoch), lr: 0.000475
2024-07-11 11:57:29.754154: step 19456/21280 (epoch 64/70), loss = 0.426514 (22.486 sec/epoch), lr: 0.000475
2024-07-11 11:57:52.246726: step 19760/21280 (epoch 65/70), loss = 0.424930 (22.493 sec/epoch), lr: 0.000475
2024-07-11 11:58:14.723626: step 20064/21280 (epoch 66/70), loss = 0.416696 (22.477 sec/epoch), lr: 0.000475
2024-07-11 11:58:37.208587: step 20368/21280 (epoch 67/70), loss = 0.418395 (22.485 sec/epoch), lr: 0.000475
2024-07-11 11:58:59.678225: step 20672/21280 (epoch 68/70), loss = 0.413643 (22.470 sec/epoch), lr: 0.000475
2024-07-11 11:59:22.152175: step 20976/21280 (epoch 69/70), loss = 0.412195 (22.474 sec/epoch), lr: 0.000475
2024-07-11 11:59:44.624989: step 21280/21280 (epoch 70/70), loss = 0.410982 (22.473 sec/epoch), lr: 0.000475
Evaluating on dev set...
...........................
source: 	 mrr: 0.033388	 ndcg_5: 0.0220	 ndcg_10: 0.0328	 hit@1:0.009413	 hit@5:0.0348	 hit@10: 0.0681
target: 	 mrr: 0.039400	 ndcg_5: 0.0315	 ndcg_10: 0.0415	 hit@1:0.009756	 hit@5:0.0523	 hit@10: 0.0836
epoch 70: train_loss = 0.410982, source_hit@10 = 0.0681, source_ndcg@10 = 0.0328, target_hit@10 = 0.0836, target_ndcg@10 = 0.0415

